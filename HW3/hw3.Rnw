%% LyX 2.0.5.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(replace.assign=TRUE,width=90)
@


\title{Stat 2025- HW 3}
\author{Michael Discenza}

\maketitle

I first import the data and conduct some exploratory data analysis 
<<start,message=FALSE>>=
ToyExample <- read.table("http://www.stat.columbia.edu/~madigan/W2025/data/ToyExample.dat", header=TRUE)
attach(ToyExample)
@
Then I plot the distribution of each predictor for responses y=0 and y=1.  This provides some intuition about how the features might help us differentiate between records that are classfied by 0 and 1.
<<loadinglibs,message=FALSE, echo=FALSE>>=
library(BMA)
library(ggplot2)
library(popbio)
@

<<exploratorydataanalysis,message=FALSE, echo=FALSE, fig.width=5, fig.height=3>>=
ggplot(ToyExample, aes(x1, fill = as.factor(y))) +
    geom_density(alpha = 0.2) + 
    labs(title = expression("Comparing x1 density for Y==0 and Y==1"))
ggplot(ToyExample, aes(x2, fill = as.factor(y))) +
    geom_density(alpha = 0.2) + 
    labs(title = expression("Comparing x2 density for Y==0 and Y==1"))
ggplot(ToyExample, aes(x3, fill = as.factor(y))) +
    geom_density(alpha = 0.2) + 
    labs(title = expression("Comparing x3 density for Y==0 and Y==1"))
@
Before moving into fitting complex logistic regression models with multiple predictors, I fit models with single predictors and examine residual plots to see if there are patterns that indicate a transformation of any of the predictors might be useful.  Instead of plotting the actual data points along y=0 and y=1, I plot the distributions of the x variables along those horizontal lines so that the relationships can be seen more clearly.


<<singlevariablemodel, fig.width=7, fig.height=4 >>=
testmodel1 <- glm(y ~ x1, family=binomial, data=ToyExample)
pred1 <- predict(testmodel1, type = "response") 
par(mfrow = c(1, 2))
logi.hist.plot(ToyExample$x1,ToyExample$y,boxp=FALSE,type="hist",col="gray",
               main="Fitted Logistic Regression with x1 as Predictor")
plot(testmodel1$residuals~ToyExample$x1, main="Residuals vs. x1")
#x2
testmodel2 <- glm(y ~ x2, family=binomial, data=ToyExample)
pred2 <- predict(testmodel2, type = "response")
par(mfrow = c(1, 2))
logi.hist.plot(x2,y,boxp=FALSE,type="hist",col="gray", 
               main="Fitted Logistic Regression with x2 as Predictor")
plot(testmodel2$residuals~ToyExample$x2, main="Residuals vs. x2")
#3
testmodel3a <- glm(ToyExample$y~ToyExample$x3, family=binomial, data=ToyExample)
pred3 <- predict(testmodel3a, type = "response")
par(mfrow = c(1, 2))
logi.hist.plot(ToyExample$x3,ToyExample$y,boxp=FALSE,type="hist",col="gray")
plot(testmodel3a$residuals~ToyExample$x3, main="Residuals vs. x3")
@

We can confirm that there are indeed issues fitting a logistic regression model to x3.  To get rid of the problem with the pattern of the residuals, we transform x3 and instead fit the model to log of x3:
<<logx3, fig.width=7, fig.height=4  >>=
ggplot(ToyExample, aes(log(x3), fill = as.factor(y))) +
    geom_density(alpha = 0.2) + 
    labs(title = expression("Comparing log(x3) density for Y==0 and Y==1"))
testmodel3b <- glm(y~log(x3), family=binomial, data=ToyExample)
par(mfrow = c(1, 2))
logi.hist.plot(log(x3),y,boxp=FALSE,type="hist",col="gray")
plot(testmodel3b$residuals~log(x3), main="Residuals vs. log(x3)")

ToyExample$x3 <- log(ToyExample$x3)
colnames(ToyExample)[4] <- "logx3"
@

Having established that each of the predictors indivdually are acceptable to use in logistic regression, I then proceed to model selection by searching the model space of the combinations of x1, x2, and the log of x3.  To start this procedure, I define functions to enumerate all possible models, fit theses models, and retunrn metrics for their evaluation


<<apply_formula,tidy=FALSE, results='hide', message=FALSE, warning=FALSE>>=
all_possible_models <- function(x.column.names, y.col.name){
    Cols <- x.column.names
    n <- length(Cols)
    id <- unlist(
        lapply(1:n,
               function(i)combn(1:n,i,simplify=F)
        )
        ,recursive=F)
    Formulas <- sapply(id,function(i)
        paste(y.col.name, "~",paste(Cols[i],collapse="+"))
    )
}
library(nlme)
apply_formula <- function(model, cv=5){ #make the default value for this function 5
    fitmodel <- glm(model, family=binomial, data=ToyExample)
    #cv.model <- CVlm(ToyExample,glm(model, family=binomial, data=ToyExample),m=cv)
    #cvmse <-mean((cv.model$sale.price - cv.model$cvpred)^2) 
    AIC <- fitmodel$aic
    BIC <- BIC(fitmodel)
    result <- c(AIC, BIC)
    return (result)
}

FORMULAS <- all_possible_models(
    x.column.names=colnames(ToyExample[2:4]),
    y.col.name=colnames(ToyExample[1])
    )

FORMULAS[8] <- "y ~ 1" # add the null model
crit <- as.data.frame(lapply(X=FORMULAS, apply_formula))
names(crit) = NULL
model.selection <- cbind(FORMULAS, t(crit))
colnames(model.selection)[2] <- "AIC"
colnames(model.selection)[3] <- "BIC"
@

<<printresults>>=
model.selection
@


\section{What is the best model according to AIC?}
The best model is the model that includes all of the predictors, y ~ x1+x2+logx3.

\section{What is the best model according to BIC?}
The best model is y ~ x1+logx3, which makes sense because tends to chose smaller models.
e
\section{What is the best model using lasso?}
<<lasso,message=FALSE, warning=FALSE>>=
library(glmnet)
glmnetModel <- glmnet(x=as.matrix(ToyExample[,2:4]),y=ToyExample[,1], family="binomial")
#plot(glmnetModel)
my.cv <- cv.glmnet(x=as.matrix(ToyExample[,2:4]),y=ToyExample[,1],family="binomial")
predict(glmnetModel,type="coef",s=my.cv$lambda.min) # returns the coefficents for the best model
@
Here we see that the best model fit by lasso is one that includes x1, x2, and log(x3) and they are indeed smaller than the coefficeints that are fit by the regular logistic regression model fit to the same variables because of the shrinkage effect of lasso.
<<lassoresult>>=
glm(y~x1+x2+logx3,data=ToyExample, family="binomial")$coefficients
@
\section{Are there any transformations of the xâ€™s that appear to help? Explain.}
We just calculate the BIC and AIC for all of the models that would be fit on the non-log transformed x3 variable and note that these models have higher AICs and BICs, indicating that the transformed models are more accurate.
<<fittingmodelwithoutlog, results='hide', message=FALSE, warning=FALSE, tidy=FALSE, echo=FALSE>>=
ToyExampleNoLog <- read.table("http://www.stat.columbia.edu/~madigan/W2025/data/ToyExample.dat", 
                         header=TRUE)
apply_formula <- function(model, cv=5){ #make the default value for this function 5
    fitmodel <- glm(model, family=binomial, data=ToyExampleNoLog)

    AIC <- fitmodel$aic
    BIC <- BIC(fitmodel)
    result <- c(AIC, BIC)
    return (result)
}
FORMULAS2 <- all_possible_models(
    x.column.names=colnames(ToyExampleNoLog[2:4]),
    y.col.name=colnames(ToyExampleNoLog[1])
    )
FORMULAS2[8] <- "y ~ 1" # add the null model

crit2 <- as.data.frame(lapply(X=FORMULAS2, apply_formula))
names(crit2) = NULL
model.selection2 <- cbind(FORMULAS2, t(crit2))
colnames(model.selection2)[2] <- "AIC"
colnames(model.selection2)[3] <- "BIC"
colnames(model.selection)[2] <- "AIC-transformation"
colnames(model.selection)[3] <- "BIC-transformation"
comp <- cbind(model.selection,model.selection2[,2:3])
@
<<resultsofcomparison>>=
comp
@





\end{document}